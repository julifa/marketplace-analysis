{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark py4j\n"
      ],
      "metadata": {
        "id": "ga5XljxSjPBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Crear sesion de Spark\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('nubi_chall')\\\n",
        "        .getOrCreate()\n"
      ],
      "metadata": {
        "id": "wF8xnNObaX9C"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os import truncate\n",
        "from google.colab import drive\n",
        "from pyspark.sql.types import  StructField, DoubleType, IntegerType, StringType, StructType, LongType\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import col, when, sum, count, rank, lit\n",
        "\n",
        "#Montar Google Drive\n",
        "drive.mount('/content/drive')\n",
        "#spark.conf.set(\"spark.sql.parquet.int64AsDouble\", \"true\")\n",
        "\n",
        "\n",
        "\n",
        "#1. Lectura de los archivos de los primeros 7 días del mes. En caso de haber días faltantes en el período analizado, logear dichas fechas para identificar cuáles son los faltantess.\n",
        "\n",
        "#Ruta base de las carpetas\n",
        "df1 = spark.read.parquet(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240701\")\n",
        "df2 = spark.read.parquet(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240702\")\n",
        "df3 = spark.read.parquet(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240703\")\n",
        "df4 = spark.read.parquet(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240705\")\n",
        "df5 = spark.read.parquet(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240706\")\n",
        "df6 = spark.read.parquet(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240707\")\n",
        "\n",
        "\n",
        "#(\"/content/drive/MyDrive/compressedData/year=2024/month=07/day=20240705/sellerId=999888777/\")\n",
        "\n",
        "#df1.printSchema()\n",
        "#df2.printSchema()\n",
        "#df3.printSchema()\n",
        "#df4.printSchema()\n",
        "#df5.printSchema()\n",
        "#df6.printSchema()\n",
        "\n",
        "#Unimos los dataframes posterior a verificacion de datos de manera independiente\n",
        "dfUnida = df2.unionByName(df3, allowMissingColumns=True) \\\n",
        "                 .unionByName(df4, allowMissingColumns=True) \\\n",
        "                 .unionByName(df5, allowMissingColumns=True) \\\n",
        "                 .unionByName(df1, allowMissingColumns=True) \\\n",
        "                 .unionByName(df6, allowMissingColumns=True)\n",
        "\n",
        "#Establezco limite de 30 registros ya que no supera dicho numero\n",
        "dfUnida.show(30)\n",
        "\n",
        "\n",
        "#dfUnida.printSchema()  La idea era verificar el schema una vez unido para que no haya problema con el tipo de dato en columnas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXq0hYLXkRor",
        "outputId": "8f698332-acd0-49ac-8947-cb49ec7376c6"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "+--------+---------+-------+-----+---------+-----------+--------------------+\n",
            "|currency|       id|  price|sales| sellerId|   category|               title|\n",
            "+--------+---------+-------+-----+---------+-----------+--------------------+\n",
            "|     ARS|MLA123456|  150.0|    2|999888777|       NULL|                NULL|\n",
            "|     ARS|MLA654321|   30.0|    1|999888777|       NULL|                NULL|\n",
            "|     ARS|MLA333333|   20.0|   15|444555666|       NULL|                NULL|\n",
            "|     ARS|MLA333444|   10.0|    3|444555666|       NULL|                NULL|\n",
            "|     ARS|MLA123456|  160.0|    5|999888777|Electronics|                NULL|\n",
            "|     ARS|MLA654321|   60.0|    1|999888777|Electronics|                NULL|\n",
            "|     ARS|MLA333333|   50.0|   16|444555666|Electronics|                NULL|\n",
            "|     ARS|MLA121212|   10.0|    6|444555666|Electronics|                NULL|\n",
            "|     ARS|MLA999000|  150.0|    2|999888777|      Tools|                NULL|\n",
            "|     ARS|MLA991199|   90.0|    2|999888777|      Shoes|                NULL|\n",
            "|     ARS|MLA991188|   60.0|    3|999888777|      Shoes|                NULL|\n",
            "|     ARS|MLA555555|   30.0|    5|444555666|      Shoes|                NULL|\n",
            "|     ARS|MLA676767|   20.0|    8|444555666|      Shoes|                NULL|\n",
            "|     ARS|MLA111111|   75.0|  121|111222333|      Tools|                NULL|\n",
            "|     ARS|MLA777000| 9001.0|    1|999888777|      Shoes|Nike jordan super...|\n",
            "|     ARS|MLA888999| 1009.0|    7|999888777|Electronics|samsun phone 100%...|\n",
            "|     ARS|MLA222111|   50.0|    8|444555666|      Shoes|   Adidas [redacted]|\n",
            "|     ARS|MLA222333|   60.0|    9|444555666|Electronics|       Motorla big F|\n",
            "|     ARS|MLA777000| 9001.0|    8|999888777|      Shoes|Nike jordan super...|\n",
            "|     ARS|MLA888999| 1009.0|    3|999888777|Electronics|samsun phone 100%...|\n",
            "|     ARS|MLA123456|  150.0|    3|999888777|       NULL|                NULL|\n",
            "|     ARS|MLA333333|   20.0|    5|444555666|       NULL|                NULL|\n",
            "|     ARS|MLA777000| 9001.0|    2|999888777|      Shoes|Nike jordan super...|\n",
            "|     ARS|MLA888999|15000.0|    1|999888777|Electronics|           iPhone 14|\n",
            "|     ARS|MLA222111|   68.0|    8|444555666|      Shoes|   Adidas [redacted]|\n",
            "|     ARS|MLA222333|   70.0|   12|444555666|Electronics|       Motorla big F|\n",
            "+--------+---------+-------+-----+---------+-----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#dfRevisionDias = spark.read.parquet('/content/drive/MyDrive/compressedData/year=2024/month=07')\n",
        "#Ruta base de las carpetas\n",
        "base_path = '/content/drive/MyDrive/compressedData/year=2024/month=07/'\n",
        "\n",
        "#Lista de días esperados del 1 al 7\n",
        "diasSemana = [f'day=2024070{dia}' for dia in range(1, 8)]\n",
        "\n",
        "#Lista de carpetas presentes en la ruta\n",
        "diasEncontrados = [folder for folder in os.listdir(base_path) if folder.startswith('day=')]\n",
        "\n",
        "#Comparar días esperados con los que existen\n",
        "diasFaltantes = [dia for dia in diasSemana if dia not in diasEncontrados]\n",
        "\n",
        "#Mostrar los resultados\n",
        "if diasFaltantes:\n",
        "    print(f\"Faltan los siguientes días: {diasFaltantes}\")\n",
        "else:\n",
        "    print(\"No faltan días.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG9BhGgf3ZbI",
        "outputId": "3eca9732-5a39-4fbc-8459-f1fc4d644d10"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Faltan los siguientes días: ['day=20240704']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#2. Agregar una columna al dataframe leído en el punto 1 que calcule la facturación de cada venta (cada fila es una venta).\n",
        "dfUnida = dfUnida.withColumn(\"GMV\", when(col(\"sales\").isNull() | col(\"price\").isNull(), 0)\n",
        "                                    .otherwise(col(\"sales\") * col(\"price\")))\n",
        "\n",
        "dfUnida.show(30)\n",
        "dfUnida.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMQP4VqRNPLQ",
        "outputId": "770b9b72-b6cb-4fc4-bc96-14d3d4d37ed7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-----+---------+-----------+--------------------+-------+\n",
            "|currency|       id|  price|sales| sellerId|   category|               title|    GMV|\n",
            "+--------+---------+-------+-----+---------+-----------+--------------------+-------+\n",
            "|     ARS|MLA123456|  150.0|    2|999888777|       NULL|                NULL|  300.0|\n",
            "|     ARS|MLA654321|   30.0|    1|999888777|       NULL|                NULL|   30.0|\n",
            "|     ARS|MLA333333|   20.0|   15|444555666|       NULL|                NULL|  300.0|\n",
            "|     ARS|MLA333444|   10.0|    3|444555666|       NULL|                NULL|   30.0|\n",
            "|     ARS|MLA123456|  160.0|    5|999888777|Electronics|                NULL|  800.0|\n",
            "|     ARS|MLA654321|   60.0|    1|999888777|Electronics|                NULL|   60.0|\n",
            "|     ARS|MLA333333|   50.0|   16|444555666|Electronics|                NULL|  800.0|\n",
            "|     ARS|MLA121212|   10.0|    6|444555666|Electronics|                NULL|   60.0|\n",
            "|     ARS|MLA999000|  150.0|    2|999888777|      Tools|                NULL|  300.0|\n",
            "|     ARS|MLA991199|   90.0|    2|999888777|      Shoes|                NULL|  180.0|\n",
            "|     ARS|MLA991188|   60.0|    3|999888777|      Shoes|                NULL|  180.0|\n",
            "|     ARS|MLA555555|   30.0|    5|444555666|      Shoes|                NULL|  150.0|\n",
            "|     ARS|MLA676767|   20.0|    8|444555666|      Shoes|                NULL|  160.0|\n",
            "|     ARS|MLA111111|   75.0|  121|111222333|      Tools|                NULL| 9075.0|\n",
            "|     ARS|MLA777000| 9001.0|    1|999888777|      Shoes|Nike jordan super...| 9001.0|\n",
            "|     ARS|MLA888999| 1009.0|    7|999888777|Electronics|samsun phone 100%...| 7063.0|\n",
            "|     ARS|MLA222111|   50.0|    8|444555666|      Shoes|   Adidas [redacted]|  400.0|\n",
            "|     ARS|MLA222333|   60.0|    9|444555666|Electronics|       Motorla big F|  540.0|\n",
            "|     ARS|MLA777000| 9001.0|    8|999888777|      Shoes|Nike jordan super...|72008.0|\n",
            "|     ARS|MLA888999| 1009.0|    3|999888777|Electronics|samsun phone 100%...| 3027.0|\n",
            "|     ARS|MLA123456|  150.0|    3|999888777|       NULL|                NULL|  450.0|\n",
            "|     ARS|MLA333333|   20.0|    5|444555666|       NULL|                NULL|  100.0|\n",
            "|     ARS|MLA777000| 9001.0|    2|999888777|      Shoes|Nike jordan super...|18002.0|\n",
            "|     ARS|MLA888999|15000.0|    1|999888777|Electronics|           iPhone 14|15000.0|\n",
            "|     ARS|MLA222111|   68.0|    8|444555666|      Shoes|   Adidas [redacted]|  544.0|\n",
            "|     ARS|MLA222333|   70.0|   12|444555666|Electronics|       Motorla big F|  840.0|\n",
            "+--------+---------+-------+-----+---------+-----------+--------------------+-------+\n",
            "\n",
            "root\n",
            " |-- currency: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- sales: long (nullable = true)\n",
            " |-- sellerId: integer (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- GMV: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.\n",
        "\n",
        "#a- Calcular la facturación total de cada seller.\n",
        "dfFacturacionTotal = dfUnida.groupBy(\"sellerId\") \\\n",
        "                            .agg(sum(\"GMV\").alias(\"total_revenue\"))\n",
        "\n",
        "dfFacturacionTotal.show()\n",
        "\n",
        "#b- Unidades vendidas por item\n",
        "dfUnidadesVendidas = dfUnida.groupBy(\"id\") \\\n",
        "                        .agg(sum(\"sales\").alias(\"total_units_sold\"))\n",
        "\n",
        "dfUnidadesVendidas.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1hN0f2rQAwA",
        "outputId": "5df88540-0ed6-4bae-f923-be9d51d66752"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+\n",
            "| sellerId|total_revenue|\n",
            "+---------+-------------+\n",
            "|999888777|     126401.0|\n",
            "|444555666|       3924.0|\n",
            "|111222333|       9075.0|\n",
            "+---------+-------------+\n",
            "\n",
            "+---------+----------------+\n",
            "|       id|total_units_sold|\n",
            "+---------+----------------+\n",
            "|MLA123456|              10|\n",
            "|MLA654321|               2|\n",
            "|MLA333333|              36|\n",
            "|MLA333444|               3|\n",
            "|MLA991199|               2|\n",
            "|MLA991188|               3|\n",
            "|MLA121212|               6|\n",
            "|MLA999000|               2|\n",
            "|MLA111111|             121|\n",
            "|MLA555555|               5|\n",
            "|MLA676767|               8|\n",
            "|MLA888999|              11|\n",
            "|MLA777000|              11|\n",
            "|MLA222333|              21|\n",
            "|MLA222111|              16|\n",
            "+---------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Asignar un ranking a cada vendedor según los totales calculados en el punto 3 a. Cada vendedor debe obtener una posición\n",
        "#  en el ranking según el total facturado, siendo la posición 1 la de mayor facturación. Guardar el ranking en formato de un único CSV\n",
        "\n",
        "dfFacturacionTotal_ranked = dfFacturacionTotal.withColumn(\n",
        "    \"rank\",\n",
        "    rank().over(Window.orderBy(col(\"total_revenue\").desc()))  #Ordena las filas dentro de la ventana por la columna \"total_revenue\" de mayor a menor\n",
        ")\n",
        "\n",
        "#Filtramos y seleccionamos solamente las columnas \"sellerId\" y \"rank\" para ver resultados\n",
        "dfRankingVentas = dfFacturacionTotal_ranked.select(\"sellerId\", \"rank\")\n",
        "\n",
        "#Se guarda ranking en archivo CSV, tambien estuvo en mente la descarga a maquina local del mismo pero pensandolo a gran escala no es muy optimizada la solucion.\n",
        "dfRankingVentas.coalesce(1).write.csv(\"dfRankingVentas.csv\", header=True)\n",
        "\n",
        "dfRankingVentas.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "WS4PSooQQ41J",
        "outputId": "281652ea-36be-4e64-87da-6f1c3e57ad97"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/content/dfRankingVentas.csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-e6542f87b68a>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#Se guarda ranking en archivo CSV, tambien estuvo en mente la descarga a maquina local del mismo pero pensandolo a gran escala no es muy optimizada la solucion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdfRankingVentas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dfRankingVentas.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdfRankingVentas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m         )\n\u001b[0;32m-> 1864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     def orc(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/dfRankingVentas.csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Guardar en formato parquet el dataframe del punto 2 de forma particionada por año, mes, seller.\n",
        "\n",
        "#Aca creamos las columnas año y mes\n",
        "dfUnida = dfUnida.withColumn(\"year\", lit(2024)) \\\n",
        "                         .withColumn(\"month\", lit(7))\n",
        "\n",
        "#Particionamos por año, mes, seller, esta linea podria añadirse al inicio debido a que solamente se puede ejecutar una vez\n",
        "dfUnida.write.partitionBy(\"year\", \"month\", \"sellerId\").parquet(\"combined_data.parquet\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Wu-7XQVJRgE5",
        "outputId": "19901745-2dbb-40f0-9939-6fe3f66820e4"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/content/combined_data.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-d69ec985d13a>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Particionamos por año, mes, seller, esta linea podria añadirse al inicio debido a que solamente se puede ejecutar una vez\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdfUnida\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"month\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sellerId\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"combined_data.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/combined_data.parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Code cell adicional para leer y mostrar archivo parquet particionado por año, mes, seller.\n",
        "dfParquet= spark.read.parquet(\"/content/combined_data.parquet\")\n",
        "\n",
        "#Ordenamos por sellerId de mayor a menor\n",
        "dfParquetOrdenado = dfParquet.orderBy(col(\"sellerId\").desc())\n",
        "\n",
        "dfParquetOrdenado.show(30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pAS4pxjTtO5",
        "outputId": "5f3a2cd0-1c72-4875-8f1c-92f6eaf04b65"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---------+-------+-----+-----------+--------------------+-------+----+-----+---------+\n",
            "|currency|       id|  price|sales|   category|               title|    GMV|year|month| sellerId|\n",
            "+--------+---------+-------+-----+-----------+--------------------+-------+----+-----+---------+\n",
            "|     ARS|MLA123456|  150.0|    2|       NULL|                NULL|  300.0|2024|    7|999888777|\n",
            "|     ARS|MLA123456|  160.0|    5|Electronics|                NULL|  800.0|2024|    7|999888777|\n",
            "|     ARS|MLA654321|   30.0|    1|       NULL|                NULL|   30.0|2024|    7|999888777|\n",
            "|     ARS|MLA654321|   60.0|    1|Electronics|                NULL|   60.0|2024|    7|999888777|\n",
            "|     ARS|MLA123456|  150.0|    3|       NULL|                NULL|  450.0|2024|    7|999888777|\n",
            "|     ARS|MLA999000|  150.0|    2|      Tools|                NULL|  300.0|2024|    7|999888777|\n",
            "|     ARS|MLA991199|   90.0|    2|      Shoes|                NULL|  180.0|2024|    7|999888777|\n",
            "|     ARS|MLA991188|   60.0|    3|      Shoes|                NULL|  180.0|2024|    7|999888777|\n",
            "|     ARS|MLA777000| 9001.0|    1|      Shoes|Nike jordan super...| 9001.0|2024|    7|999888777|\n",
            "|     ARS|MLA888999| 1009.0|    7|Electronics|samsun phone 100%...| 7063.0|2024|    7|999888777|\n",
            "|     ARS|MLA777000| 9001.0|    8|      Shoes|Nike jordan super...|72008.0|2024|    7|999888777|\n",
            "|     ARS|MLA888999| 1009.0|    3|Electronics|samsun phone 100%...| 3027.0|2024|    7|999888777|\n",
            "|     ARS|MLA777000| 9001.0|    2|      Shoes|Nike jordan super...|18002.0|2024|    7|999888777|\n",
            "|     ARS|MLA888999|15000.0|    1|Electronics|           iPhone 14|15000.0|2024|    7|999888777|\n",
            "|     ARS|MLA555555|   30.0|    5|      Shoes|                NULL|  150.0|2024|    7|444555666|\n",
            "|     ARS|MLA222111|   50.0|    8|      Shoes|   Adidas [redacted]|  400.0|2024|    7|444555666|\n",
            "|     ARS|MLA676767|   20.0|    8|      Shoes|                NULL|  160.0|2024|    7|444555666|\n",
            "|     ARS|MLA222333|   60.0|    9|Electronics|       Motorla big F|  540.0|2024|    7|444555666|\n",
            "|     ARS|MLA333333|   20.0|   15|       NULL|                NULL|  300.0|2024|    7|444555666|\n",
            "|     ARS|MLA222111|   68.0|    8|      Shoes|   Adidas [redacted]|  544.0|2024|    7|444555666|\n",
            "|     ARS|MLA333444|   10.0|    3|       NULL|                NULL|   30.0|2024|    7|444555666|\n",
            "|     ARS|MLA222333|   70.0|   12|Electronics|       Motorla big F|  840.0|2024|    7|444555666|\n",
            "|     ARS|MLA333333|   20.0|    5|       NULL|                NULL|  100.0|2024|    7|444555666|\n",
            "|     ARS|MLA333333|   50.0|   16|Electronics|                NULL|  800.0|2024|    7|444555666|\n",
            "|     ARS|MLA121212|   10.0|    6|Electronics|                NULL|   60.0|2024|    7|444555666|\n",
            "|     ARS|MLA111111|   75.0|  121|      Tools|                NULL| 9075.0|2024|    7|111222333|\n",
            "+--------+---------+-------+-----+-----------+--------------------+-------+----+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-data-tables\n",
        "!pip install azure-storage-blob\n",
        "!pip install python-dotenv #configurar variable de entorno"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAaG2VkpvCkr",
        "outputId": "3fc20bc9-a26c-42a0-ebac-8324db6909b0"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: azure-data-tables in /usr/local/lib/python3.10/dist-packages (12.5.0)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.29.4 in /usr/local/lib/python3.10/dist-packages (from azure-data-tables) (1.30.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from azure-data-tables) (1.9.4)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-data-tables) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-data-tables) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.29.4->azure-data-tables) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.29.4->azure-data-tables) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->azure-data-tables) (3.8)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->azure-data-tables) (6.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.29.4->azure-data-tables) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.29.4->azure-data-tables) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core<2.0.0,>=1.29.4->azure-data-tables) (2024.8.30)\n",
            "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.10/dist-packages (12.22.0)\n",
            "Requirement already satisfied: azure-core>=1.28.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (1.30.2)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (43.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (4.12.2)\n",
            "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob) (0.6.1)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.28.0->azure-storage-blob) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.1.4->azure-storage-blob) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.28.0->azure-storage-blob) (2024.8.30)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.data.tables import TableServiceClient\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "# Conexión a la cuenta de Azure Table\n",
        "#The connection string was modified to use double curly braces to properly format the f-string and access the environment variable.\n",
        "connection_string = f\"DefaultEndpointsProtocol=https;AccountName={os.getenv('AZURE_STORAGE_ACCOUNT')};AccountKey={os.getenv('AZURE_STORAGE_KEY')};EndpointSuffix=core.windows.net\"\n",
        "table_service_client = TableServiceClient.from_connection_string(conn_str=connection_string)\n",
        "\n",
        "# Crear cliente para la tabla\n",
        "table_client = table_service_client.get_table_client(table_name=\"FacturacionVentas\")\n",
        "\n",
        "# Subir cada fila de la tabla\n",
        "for row in dfUnida.collect():\n",
        "    entity = {\n",
        "        'PartitionKey': str(row['sellerId']),\n",
        "        'RowKey': str(row['id']),\n",
        "        'price': row['price'],\n",
        "        'sales': row['sales'],\n",
        "        'currency': row['currency'],\n",
        "        'category': row['category'],\n",
        "        'title': row['title'],\n",
        "        'GMV': row['GMV']\n",
        "    }\n",
        "    try:\n",
        "        # Intenta crear la entidad\n",
        "        table_client.create_entity(entity=entity)\n",
        "    except ResourceExistsError:\n",
        "        # Maneja la excepción si la entidad ya existe\n",
        "        print(f\"Entity with PartitionKey '{entity['PartitionKey']}' and RowKey '{entity['RowKey']}' already exists.\")\n",
        "        # Puedes optar por actualizar la entidad existente o simplemente ignorar el error.\n",
        "        # Para actualizar la entidad:\n",
        "        # table_client.update_entity(entity=entity)\n",
        "\n",
        "        # Consultar todas las entidades (esto puede ser un gran volumen de datos)\n",
        "# Se agrega un filtro para que la consulta retorne todas las entidades\n",
        "entities = table_client.query_entities(query_filter=\"\")\n",
        "\n",
        "for entity in entities:\n",
        "    print(entity)\n",
        "\n",
        "\n",
        "# Consultar entidades con un PartitionKey específico\n",
        "entities = table_client.query_entities(query_filter=\"PartitionKey eq '999888777'\")\n",
        "\n",
        "for entity in entities:\n",
        "    print(entity)\n",
        "\n",
        "# Consultar entidades con un filtro más complejo\n",
        "entities = table_client.query_entities(query_filter=\"PartitionKey eq '999888777' and price gt 10\")\n",
        "\n",
        "for entity in entities:\n",
        "    print(entity)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hz4_viK9u6sX",
        "outputId": "b7025821-8752-46c8-a9d2-520d486c3c7f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity with PartitionKey '999888777' and RowKey 'MLA123456' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA654321' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA333333' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA333444' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA123456' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA654321' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA333333' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA121212' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA999000' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA991199' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA991188' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA555555' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA676767' already exists.\n",
            "Entity with PartitionKey '111222333' and RowKey 'MLA111111' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA777000' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA888999' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA222111' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA222333' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA777000' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA888999' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA123456' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA333333' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA777000' already exists.\n",
            "Entity with PartitionKey '999888777' and RowKey 'MLA888999' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA222111' already exists.\n",
            "Entity with PartitionKey '444555666' and RowKey 'MLA222333' already exists.\n",
            "{'PartitionKey': '111222333', 'RowKey': 'MLA111111', 'price': 75.0, 'sales': 121, 'currency': 'ARS', 'category': 'Tools', 'GMV': 9075.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA121212', 'price': 10.0, 'sales': 6, 'currency': 'ARS', 'category': 'Electronics', 'GMV': 60.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA222111', 'price': 50.0, 'sales': 8, 'currency': 'ARS', 'category': 'Shoes', 'title': 'Adidas [redacted]', 'GMV': 400.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA222333', 'price': 60.0, 'sales': 9, 'currency': 'ARS', 'category': 'Electronics', 'title': 'Motorla big F', 'GMV': 540.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA333333', 'price': 20.0, 'sales': 15, 'currency': 'ARS', 'GMV': 300.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA333444', 'price': 10.0, 'sales': 3, 'currency': 'ARS', 'GMV': 30.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA555555', 'price': 30.0, 'sales': 5, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 150.0}\n",
            "{'PartitionKey': '444555666', 'RowKey': 'MLA676767', 'price': 20.0, 'sales': 8, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 160.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA123456', 'price': 150.0, 'sales': 2, 'currency': 'ARS', 'GMV': 300.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA654321', 'price': 30.0, 'sales': 1, 'currency': 'ARS', 'GMV': 30.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA777000', 'price': 9001.0, 'sales': 1, 'currency': 'ARS', 'category': 'Shoes', 'title': 'Nike jordan super cool', 'GMV': 9001.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA888999', 'price': 1009.0, 'sales': 7, 'currency': 'ARS', 'category': 'Electronics', 'title': 'samsun phone 100%realnofake', 'GMV': 7063.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA991188', 'price': 60.0, 'sales': 3, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 180.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA991199', 'price': 90.0, 'sales': 2, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 180.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA999000', 'price': 150.0, 'sales': 2, 'currency': 'ARS', 'category': 'Tools', 'GMV': 300.0}\n",
            "{'PartitionKey': 'sellerId', 'RowKey': 'id'}\n",
            "{'PartitionKey': 'ventas', 'RowKey': 'MLA123456', 'currency': 'ARS', 'id': 'MLA123456', 'price': 150.0, 'sales': 3, 'sellerId': 999888777, 'GMV': 450.0}\n",
            "{'PartitionKey': 'ventas', 'RowKey': 'MLA333333', 'currency': 'ARS', 'id': 'MLA333333', 'price': 20.0, 'sales': 5, 'sellerId': 444555666, 'GMV': 100.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA123456', 'price': 150.0, 'sales': 2, 'currency': 'ARS', 'GMV': 300.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA654321', 'price': 30.0, 'sales': 1, 'currency': 'ARS', 'GMV': 30.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA777000', 'price': 9001.0, 'sales': 1, 'currency': 'ARS', 'category': 'Shoes', 'title': 'Nike jordan super cool', 'GMV': 9001.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA888999', 'price': 1009.0, 'sales': 7, 'currency': 'ARS', 'category': 'Electronics', 'title': 'samsun phone 100%realnofake', 'GMV': 7063.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA991188', 'price': 60.0, 'sales': 3, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 180.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA991199', 'price': 90.0, 'sales': 2, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 180.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA999000', 'price': 150.0, 'sales': 2, 'currency': 'ARS', 'category': 'Tools', 'GMV': 300.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA123456', 'price': 150.0, 'sales': 2, 'currency': 'ARS', 'GMV': 300.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA654321', 'price': 30.0, 'sales': 1, 'currency': 'ARS', 'GMV': 30.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA777000', 'price': 9001.0, 'sales': 1, 'currency': 'ARS', 'category': 'Shoes', 'title': 'Nike jordan super cool', 'GMV': 9001.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA888999', 'price': 1009.0, 'sales': 7, 'currency': 'ARS', 'category': 'Electronics', 'title': 'samsun phone 100%realnofake', 'GMV': 7063.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA991188', 'price': 60.0, 'sales': 3, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 180.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA991199', 'price': 90.0, 'sales': 2, 'currency': 'ARS', 'category': 'Shoes', 'GMV': 180.0}\n",
            "{'PartitionKey': '999888777', 'RowKey': 'MLA999000', 'price': 150.0, 'sales': 2, 'currency': 'ARS', 'category': 'Tools', 'GMV': 300.0}\n"
          ]
        }
      ]
    }
  ]
}